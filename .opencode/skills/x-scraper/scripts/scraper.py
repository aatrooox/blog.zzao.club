#!/usr/bin/env python3
import asyncio
import json
import re
import sys
from pathlib import Path
from playwright.async_api import async_playwright

DEFAULT_COOKIE_FILE = "/tmp/x_cookies_pw.json"


async def scrape_x_posts(
    username: str, count: int = 5, cookie_file: str = DEFAULT_COOKIE_FILE
):
    if not Path(cookie_file).exists():
        print(f"âŒ Cookieæ–‡ä»¶ä¸å­˜åœ¨: {cookie_file}")
        print("\nè¯·å…ˆå¯¼å‡ºCookieå¹¶è½¬æ¢æ ¼å¼")
        print("å‚è€ƒ: .opencode/skills/x-scraper/SKILL.md")
        sys.exit(1)

    with open(cookie_file, "r") as f:
        cookies = json.load(f)

    print(f"âœ… åŠ è½½äº† {len(cookies)} ä¸ªCookies")

    async with async_playwright() as p:
        print("ğŸš€ å¯åŠ¨Chromiumæµè§ˆå™¨...")

        browser = await p.chromium.launch(
            headless=False, args=["--no-sandbox", "--disable-setuid-sandbox"]
        )

        context = await browser.new_context(viewport={"width": 1920, "height": 1080})

        await context.add_cookies(cookies)
        print("ğŸª Cookieså·²æ³¨å…¥")

        page = await context.new_page()

        url = f"https://x.com/{username}/with_replies"
        print(f"ğŸŒ è®¿é—® {url}...")

        try:
            await page.goto(url, wait_until="domcontentloaded", timeout=30000)
            await page.wait_for_timeout(5000)

            try:
                await page.wait_for_selector(
                    'article[data-testid="tweet"]', timeout=15000
                )
            except:
                print("âš ï¸  with_replies è¶…æ—¶ï¼Œå°è¯•ä¸»é¡µ...")
                await page.goto(
                    f"https://x.com/{username}",
                    wait_until="domcontentloaded",
                    timeout=30000,
                )
                await page.wait_for_timeout(5000)
                await page.wait_for_selector(
                    'article[data-testid="tweet"]', timeout=10000
                )

            print("ğŸ“œ åŠ è½½å¸–å­...")

            for i in range(12):
                await page.mouse.wheel(0, 1000)
                await page.wait_for_timeout(2000)
                print(f"ğŸ“œ æ»šåŠ¨åŠ è½½... ({i + 1}/12)")

            articles = await page.query_selector_all('article[data-testid="tweet"]')
            print(f"âœ… æ‰¾åˆ° {len(articles)} æ¡å¸–å­")

            if not articles:
                print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°å¸–å­ï¼Œå¯èƒ½Cookieå·²è¿‡æœŸ")
                await browser.close()
                return []

            posts = []

            for idx, article in enumerate(articles[:count]):
                try:
                    time_elem = await article.query_selector("time")
                    post_time = (
                        await time_elem.get_attribute("datetime")
                        if time_elem
                        else "Unknown"
                    )

                    post_link = "Unknown"
                    post_id = "Unknown"

                    if time_elem:
                        link_elem = await time_elem.evaluate_handle(
                            'node => node.closest("a")'
                        )
                        if link_elem and link_elem.as_element():
                            href = await link_elem.as_element().get_attribute("href")
                            if href:
                                post_link = (
                                    f"https://x.com{href}"
                                    if href.startswith("/")
                                    else href
                                )
                                match = re.search(r"/status/(\d+)", href)
                                if match:
                                    post_id = match.group(1)

                    text_elem = await article.query_selector(
                        '[data-testid="tweetText"]'
                    )
                    text_content = await text_elem.inner_text() if text_elem else ""

                    views_elem = await article.query_selector(
                        '[href$="/analytics"] span'
                    )
                    views = (
                        (await views_elem.inner_text()).strip() if views_elem else "N/A"
                    )

                    like_elem = await article.query_selector(
                        '[data-testid="like"] span'
                    )
                    likes = (await like_elem.inner_text()).strip() if like_elem else "0"

                    retweet_elem = await article.query_selector(
                        '[data-testid="retweet"] span'
                    )
                    retweets = (
                        (await retweet_elem.inner_text()).strip()
                        if retweet_elem
                        else "0"
                    )

                    reply_elem = await article.query_selector(
                        '[data-testid="reply"] span'
                    )
                    replies = (
                        (await reply_elem.inner_text()).strip() if reply_elem else "0"
                    )

                    post = {
                        "index": idx + 1,
                        "username": username,
                        "postId": post_id,
                        "publishTime": post_time,
                        "postLink": post_link,
                        "textContent": text_content,
                        "views": views or "N/A",
                        "likes": likes or "0",
                        "retweets": retweets or "0",
                        "replies": replies or "0",
                    }

                    posts.append(post)
                    print(f"âœ“ æå–å¸–å­ {idx + 1} (ID: {post_id})")

                except Exception as e:
                    print(f"âœ— æå–å¸–å­ {idx + 1} å¤±è´¥: {e}")

            await browser.close()
            return posts

        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
            await browser.close()
            raise


def format_output(posts):
    print("\n" + "=" * 80)
    print("ğŸ“Š ç»“æœ")
    print("=" * 80 + "\n")

    for post in posts:
        print(f"ã€å¸–å­ {post['index']}ã€‘")
        print(f"ğŸ”— é“¾æ¥: {post['postLink']}")
        print(f"ğŸ“… æ—¶é—´: {post['publishTime']}")
        print(
            f"ğŸ“ å†…å®¹: {post['textContent'][:100]}{'...' if len(post['textContent']) > 100 else ''}"
        )
        print(f"ğŸ“Š æ•°æ®:")
        print(f"   ğŸ‘€ æµè§ˆ: {post.get('views', 'N/A')}")
        print(f"   â¤ï¸  ç‚¹èµ: {post.get('likes', '0')}")
        print(f"   ğŸ” è½¬å‘: {post.get('retweets', '0')}")
        print(f"   ğŸ’¬ å›å¤: {post.get('replies', '0')}")
        print("-" * 80)


async def main():
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python3 scraper.py <ç”¨æˆ·å> [æ•°é‡] [--cookie-file <è·¯å¾„>]")
        print("\nç¤ºä¾‹:")
        print("  python3 scraper.py dotey 15")
        print("  python3 scraper.py elonmusk 20 --cookie-file /path/to/cookies.json")
        print("\nå‰æ: éœ€è¦å…ˆå¯¼å‡ºå¹¶è½¬æ¢Cookie")
        print("å‚è€ƒ: .opencode/skills/x-scraper/SKILL.md")
        sys.exit(1)

    # Parse arguments
    username = sys.argv[1].lstrip("@")
    count = 10
    cookie_file = DEFAULT_COOKIE_FILE

    # Parse count and optional flags
    i = 2
    while i < len(sys.argv):
        if sys.argv[i] == "--cookie-file" and i + 1 < len(sys.argv):
            cookie_file = sys.argv[i + 1]
            i += 2
        else:
            # Assume it's the count
            count = int(sys.argv[i])
            i += 1

    print(f"ğŸ¯ ç›®æ ‡: @{username}")
    print(f"ğŸ“Œ æ•°é‡: {count} æ¡å¸–å­")
    print(f"ğŸª Cookieæ–‡ä»¶: {cookie_file}\n")

    try:
        posts = await scrape_x_posts(username, count, cookie_file)

        if not posts:
            print("\nâš ï¸  æœªè·å–åˆ°å¸–å­")
            sys.exit(1)

        format_output(posts)

        print("\n" + "=" * 80)
        print("ğŸ“„ JSON è¾“å‡º")
        print("=" * 80)
        print(json.dumps(posts, ensure_ascii=False, indent=2))

        output_file = f"/tmp/x_{username}_posts.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(posts, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… æŠ“å–äº† {len(posts)} æ¡å¸–å­")
        print(f"ğŸ’¾ ä¿å­˜åˆ°: {output_file}")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  å·²ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
